import math

import torch
import torch.nn.functional as F

from .base import SampleScheduler


class LinearSampleScheduler(SampleScheduler):
    """
    Simple mask that performs random covering based on Bernoulli distribution, with
    p linear decreasing from 1 to 0.
    """

    def __init__(self, num_steps: int) -> None:
        """
        Initialize the scheduler with the given number of steps.

        Args:
            num_steps (int): The total number of steps for the scheduler.
        """
        super().__init__()
        self._num_steps = num_steps

    def sample(  # type: ignore
        self, step: int, prev_mask: torch.Tensor, _: torch.Tensor
    ) -> torch.Tensor:
        if step > self._num_steps:
            raise ValueError("Step must be less than the number of steps.")
        uncover_probability = step / self._num_steps
        new_mask = torch.bernoulli(torch.ones_like(prev_mask) * uncover_probability)
        return new_mask

    @property
    def num_steps(self) -> int:
        """
        Returns the number of steps in the sampling process.

        Returns:
            int: The number of steps.
        """
        return self._num_steps


class LinearEntropyBatchSampleScheduler(SampleScheduler):
    """
    A linear sample scheduler that performs samples from batch, like the best
    one described in the LLaDa paper.
    """

    def __init__(
        self, batch_length: int, steps_per_batch: int, seq_length: int
    ) -> None:
        """
        Initialize the scheduler with the given batch length and steps per batch.

        Args:
            batch_length (int): The length of each batch.
            steps_per_batch (int): The number of steps in each batch.
        """
        super().__init__()
        self._batch_length = batch_length
        self._steps_per_batch = steps_per_batch
        self._seq_length = seq_length

    def sample(
        self, step: int, prev_mask: torch.Tensor, logits: torch.Tensor
    ) -> torch.Tensor:
        new_mask = prev_mask.clone()

        attended_logit_idx_start = (step // self._steps_per_batch) * self._batch_length
        attended_logit_idx_end = attended_logit_idx_start + self._batch_length

        attended_logits = logits[:, attended_logit_idx_start:attended_logit_idx_end, :]
        attended_masks = new_mask[
            :, attended_logit_idx_start:attended_logit_idx_end
        ].clone()
        if attended_logits.numel() == 0 or attended_masks.numel() == 0:
            return new_mask
        attended_entropy = (
            -F.softmax(attended_logits, dim=-1) * F.log_softmax(attended_logits, dim=-1)
        ).sum(dim=-1)
        _, sorted_indices = attended_entropy.sort(dim=1, descending=True)
        tokens_to_uncover = math.ceil(
            (step % self._steps_per_batch + 1)
            * (self._batch_length / self._steps_per_batch)
        )
        k = min(tokens_to_uncover, attended_masks.shape[1])
        if k > 0:
            b = (
                torch.arange(attended_masks.shape[0], device=attended_masks.device)
                .unsqueeze(1)
                .expand(-1, k)
            )
            topk = sorted_indices[:, :k]
            attended_masks[b, topk] = True
            new_mask[:, attended_logit_idx_start:attended_logit_idx_end] = (
                attended_masks
            )

        return new_mask

    @property
    def num_steps(self) -> int:
        """
        Returns the number of steps in the sampling process.

        Returns:
            int: The number of steps.
        """
        return self._seq_length // self._batch_length * self._steps_per_batch
